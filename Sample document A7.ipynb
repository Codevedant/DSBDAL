{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c351b378-b9ad-4cac-aff0-810704984419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14ffbc70-66e6-4660-9121-f6e17f05a4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. \n",
      "Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n",
      "\n",
      "This sample TXT file is provided by Sample-Files.com. Visit us for more sample files and resources.\n"
     ]
    }
   ],
   "source": [
    "with open(\"simple.txt\", \"r\", encoding=\"utf-8-sig\") as file:\n",
    "    document = file.read()\n",
    "    print(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e3410bc-a844-41a9-87fc-6762e553185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "372c0eae-eda7-4992-b102-af9dc7f2636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fbe9f63-9f70-456a-9db0-52ce786ec6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vedan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b80ad75-ce7c-4368-b3de-a5873bd30931",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\vedan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vedan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vedan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vedan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\vedan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt.download('punkt_tab') \n",
    "nt.download('stopwords') \n",
    "nt.download('wordnet') \n",
    "nt.download('averaged_perceptron_tagger')\n",
    "nt.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "948b54bc-04ff-4837-8d8a-6cc7ea95c148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens after Stop Word Removal:\n",
      "\n",
      " ['Lorem', 'ipsum', 'dolor', 'sit', 'amet', ',', 'consectetur', 'adipiscing', 'elit', '.', 'Sed', 'eiusmod', 'tempor', 'incididunt', 'ut', 'labore', 'et', 'dolore', 'magna', 'aliqua', '.', 'sample', 'TXT', 'file', 'provided', 'Sample-Files.com', '.', 'Visit', 'us', 'sample', 'files', 'resources', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "tokens = word_tokenize(document) \n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words] \n",
    "print(\"\\nTokens after Stop Word Removal:\\n\\n\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f6c171c-050f-4ad7-8597-df0b8b41e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from tabulate import tabulate \n",
    "sentences = sent_tokenize(document) \n",
    "df = pd.DataFrame({'Original Sentence': sentences}) \n",
    "stemmer = PorterStemmer() \n",
    "def stem_words(sentence): \n",
    "    words = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "231b007b-359b-403a-9509-b4471d2d3868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed DataFrame (Stemmed Sentences):\n",
      "\n",
      "+----+--------------------------------------------------------------------+------------------------------------------------------------------+\n",
      "|    | Original Sentence                                                  | Stemmed Sentence                                                 |\n",
      "+====+====================================================================+==================================================================+\n",
      "|  0 | Lorem ipsum dolor sit amet, consectetur adipiscing elit.           | lorem ipsum dolor sit amet, consectetur adipisc elit.            |\n",
      "+----+--------------------------------------------------------------------+------------------------------------------------------------------+\n",
      "|  1 | Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. | sed do eiusmod tempor incididunt ut labor et dolor magna aliqua. |\n",
      "+----+--------------------------------------------------------------------+------------------------------------------------------------------+\n",
      "|  2 | This sample TXT file is provided by Sample-Files.com.              | thi sampl txt file is provid by sample-files.com.                |\n",
      "+----+--------------------------------------------------------------------+------------------------------------------------------------------+\n",
      "|  3 | Visit us for more sample files and resources.                      | visit us for more sampl file and resources.                      |\n",
      "+----+--------------------------------------------------------------------+------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(sentence):\n",
    "    words = sentence.split()\n",
    "    return ' '.join([stemmer.stem(word) for word in words])\n",
    "\n",
    "df['Stemmed Sentence'] = df['Original Sentence'].apply(stem_words)\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(\"\\nProcessed DataFrame (Stemmed Sentences):\\n\")\n",
    "print(tabulate(df, headers='keys', tablefmt='grid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9ffe878a-ae39-4bd0-983a-5132dc6f0601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed DataFrame (Lemmatized Sentences):\n",
      "\n",
      "+----+--------------------------------------------------------------------+---------------------------------------------------------------------+\n",
      "|    | Original Sentence                                                  | Lemmatized Sentence                                                 |\n",
      "+====+====================================================================+=====================================================================+\n",
      "|  0 | Lorem ipsum dolor sit amet, consectetur adipiscing elit.           | Lorem ipsum dolor sit amet , consectetur adipiscing elit .          |\n",
      "+----+--------------------------------------------------------------------+---------------------------------------------------------------------+\n",
      "|  1 | Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. | Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua . |\n",
      "+----+--------------------------------------------------------------------+---------------------------------------------------------------------+\n",
      "|  2 | This sample TXT file is provided by Sample-Files.com.              | This sample TXT file is provided by Sample-Files.com .              |\n",
      "+----+--------------------------------------------------------------------+---------------------------------------------------------------------+\n",
      "|  3 | Visit us for more sample files and resources.                      | Visit u for more sample file and resource .                         |\n",
      "+----+--------------------------------------------------------------------+---------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from tabulate import tabulate\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "sentences = sent_tokenize(document)\n",
    "df = pd.DataFrame({'Original Sentence': sentences})\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
    "\n",
    "df['Lemmatized Sentence'] = df['Original Sentence'].apply(lemmatize_words)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(\"\\nProcessed DataFrame (Lemmatized Sentences):\\n\")\n",
    "print(tabulate(df, headers='keys', tablefmt='grid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f40c4a5-94f8-4815-b5f5-d2557df7e212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------------------------------------------------------+---------------------------------------------------------------------+\n",
      "|    | Original Sentence                                                  | Lemmatized Sentence                                                 |\n",
      "+====+====================================================================+=====================================================================+\n",
      "|  0 | Lorem ipsum dolor sit amet, consectetur adipiscing elit.           | Lorem ipsum dolor sit amet , consectetur adipiscing elit .          |\n",
      "+----+--------------------------------------------------------------------+---------------------------------------------------------------------+\n",
      "|  1 | Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. | Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua . |\n",
      "+----+--------------------------------------------------------------------+---------------------------------------------------------------------+\n",
      "|  2 | This sample TXT file is provided by Sample-Files.com.              | This sample TXT file is provided by Sample-Files.com .              |\n",
      "+----+--------------------------------------------------------------------+---------------------------------------------------------------------+\n",
      "|  3 | Visit us for more sample files and resources.                      | Visit u for more sample file and resource .                         |\n",
      "+----+--------------------------------------------------------------------+---------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(df, headers='keys', tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "90413413-f6cb-4aed-b8d8-315c8346c468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed DataFrame:\n",
      "\n",
      "      tokenized_word\n",
      "0              Lorem\n",
      "1              ipsum\n",
      "2              dolor\n",
      "3                sit\n",
      "4               amet\n",
      "5        consectetur\n",
      "6         adipiscing\n",
      "7               elit\n",
      "8                Sed\n",
      "9                 do\n",
      "10           eiusmod\n",
      "11            tempor\n",
      "12        incididunt\n",
      "13                ut\n",
      "14            labore\n",
      "15                et\n",
      "16            dolore\n",
      "17             magna\n",
      "18            aliqua\n",
      "19              This\n",
      "20            sample\n",
      "21               TXT\n",
      "22              file\n",
      "23                is\n",
      "24          provided\n",
      "25                by\n",
      "26  Sample-Files.com\n",
      "27             Visit\n",
      "28                us\n",
      "29               for\n",
      "30              more\n",
      "31            sample\n",
      "32             files\n",
      "33               and\n",
      "34         resources\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "tokens = word_tokenize(document)\n",
    "tokens = [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "df = pd.DataFrame({'tokenized_word': tokens})\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(\"\\nProcessed DataFrame:\\n\")\n",
    "print(df.to_string(index=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ee578a5e-4029-420f-9349-6ac104df7402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ProcessedDataFrame(POSTagging):\n",
      "\n",
      "       TokenizedWord POSTag\n",
      "0              Lorem    NNP\n",
      "1              ipsum     NN\n",
      "2              dolor     NN\n",
      "3                sit     NN\n",
      "4               amet    VBD\n",
      "5        consectetur     JJ\n",
      "6         adipiscing    VBG\n",
      "7               elit     NN\n",
      "8                Sed    NNP\n",
      "9                 do    VBP\n",
      "10           eiusmod     VB\n",
      "11            tempor     VB\n",
      "12        incididunt     NN\n",
      "13                ut     JJ\n",
      "14            labore     NN\n",
      "15                et     NN\n",
      "16            dolore     NN\n",
      "17             magna     NN\n",
      "18            aliqua     IN\n",
      "19              This     DT\n",
      "20            sample     JJ\n",
      "21               TXT    NNP\n",
      "22              file     NN\n",
      "23                is    VBZ\n",
      "24          provided    VBN\n",
      "25                by     IN\n",
      "26  Sample-Files.com    NNP\n",
      "27             Visit    NNP\n",
      "28                us    PRP\n",
      "29               for     IN\n",
      "30              more    JJR\n",
      "31            sample     JJ\n",
      "32             files    NNS\n",
      "33               and     CC\n",
      "34         resources    NNS\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag \n",
    "tokens =word_tokenize(document) \n",
    "tokens =[word for word in tokens if word not in string.punctuation] \n",
    "pos_tags= pos_tag(tokens) \n",
    "df =pd.DataFrame(pos_tags,columns=['TokenizedWord', 'POSTag']) \n",
    "pd.set_option('display.max_colwidth',None) \n",
    "pd.set_option('display.max_rows',None) \n",
    "print(\"\\nProcessedDataFrame(POSTagging):\\n\") \n",
    "print(df.to_string(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ec081f9d-a808-4b33-ac44-95a1cf583d15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Representation of the Uploaded Document:\n",
      "\n",
      "                0     1     2     3\n",
      "adipiscing  0.354 0.000 0.000 0.000\n",
      "aliqua      0.000 0.302 0.000 0.000\n",
      "amet        0.354 0.000 0.000 0.000\n",
      "and         0.000 0.000 0.000 0.372\n",
      "by          0.000 0.000 0.315 0.000\n",
      "com         0.000 0.000 0.315 0.000\n",
      "consectetur 0.354 0.000 0.000 0.000\n",
      "do          0.000 0.302 0.000 0.000\n",
      "dolor       0.354 0.000 0.000 0.000\n",
      "dolore      0.000 0.302 0.000 0.000\n",
      "eiusmod     0.000 0.302 0.000 0.000\n",
      "elit        0.354 0.000 0.000 0.000\n",
      "et          0.000 0.302 0.000 0.000\n",
      "file        0.000 0.000 0.315 0.000\n",
      "files       0.000 0.000 0.248 0.293\n",
      "for         0.000 0.000 0.000 0.372\n",
      "incididunt  0.000 0.302 0.000 0.000\n",
      "ipsum       0.354 0.000 0.000 0.000\n",
      "is          0.000 0.000 0.315 0.000\n",
      "labore      0.000 0.302 0.000 0.000\n",
      "lorem       0.354 0.000 0.000 0.000\n",
      "magna       0.000 0.302 0.000 0.000\n",
      "more        0.000 0.000 0.000 0.372\n",
      "provided    0.000 0.000 0.315 0.000\n",
      "resources   0.000 0.000 0.000 0.372\n",
      "sample      0.000 0.000 0.496 0.293\n",
      "sed         0.000 0.302 0.000 0.000\n",
      "sit         0.354 0.000 0.000 0.000\n",
      "tempor      0.000 0.302 0.000 0.000\n",
      "this        0.000 0.000 0.315 0.000\n",
      "txt         0.000 0.000 0.315 0.000\n",
      "us          0.000 0.000 0.000 0.372\n",
      "ut          0.000 0.302 0.000 0.000\n",
      "visit       0.000 0.000 0.000 0.372\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "sentences = sent_tokenize(document)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)  \n",
    "\n",
    "print(\"\\nTF-IDF Representation of the Uploaded Document:\\n\")\n",
    "print(df_tfidf.round(3).T.to_string(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e286c1-9cf0-4f71-9801-a18d463f9cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
